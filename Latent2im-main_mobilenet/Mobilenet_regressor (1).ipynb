{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5wD63XR5wrz",
    "outputId": "bb109fdb-495d-4634-9db0-56b4887dcedb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sanika/.cache\\torch\\hub\\pytorch_vision_v0.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'deeplabv3_resnet101',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'fcn_resnet101',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'mobilenet_v2',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext50_32x4d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.list('pytorch/vision:v0.5.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLyEVXLUMsRt"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBIHj9EtMxHG",
    "outputId": "3d5b1e5c-7c2f-445c-b81c-d13b4f288649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "%pip install poutyne          # to install the Poutyne library\n",
    "%pip install wget             # to install the wget library in order to download data\n",
    "%pip install opencv-python    # to install the cv2 (opencv) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3BUtqRATPZ1"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torchvision>=0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mCDQYKu-MrEp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "#from poutyne import set_seeds, Model, ModelCheckpoint, CSVLogger, Experiment, StepLR\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PMT4sfdNf-J"
   },
   "source": [
    "# Downloading the CelebA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7hyL47KFUi-L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 2836386 / 2836386"
     ]
    }
   ],
   "source": [
    "data_root = \"datasets\"\n",
    "\n",
    "base_url = \"https://graal.ift.ulaval.ca/public/celeba/\"\n",
    "\n",
    "file_list = [\n",
    "    \"img_align_celeba.zip\",\n",
    "    \"list_attr_celeba.txt\",\n",
    "    \"identity_CelebA.txt\",\n",
    "    \"list_bbox_celeba.txt\",\n",
    "    \"list_landmarks_align_celeba.txt\",\n",
    "    \"list_eval_partition.txt\",\n",
    "]\n",
    "\n",
    "# Path to folder with the dataset\n",
    "dataset_folder = f\"{data_root}/celeba\"\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "for file in file_list:\n",
    "    url = f\"{base_url}/{file}\"\n",
    "    if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
    "        wget.download(url, f\"{dataset_folder}/{file}\")\n",
    "\n",
    "with zipfile.ZipFile(f\"{dataset_folder}/img_align_celeba.zip\", \"r\") as ziphandler:\n",
    "    ziphandler.extractall(dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA72Yl259TxR"
   },
   "source": [
    "# Mobilenet\n",
    "---\n",
    "Dataset = CelebA \\\n",
    "Epochs = 50 \\\n",
    "Number of binary attributes = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dVXXNBRj9TJe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# 6826/1745 = 8571\n",
    "\n",
    "import numpy as np\n",
    "#import nibabel as nib\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import ceil\n",
    "from  torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqNIGizKac8L",
    "outputId": "39be226d-c81a-4173-9e08-0500a012b3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/celeba\n",
      "datasets/celeba\\training_test_splits/\n",
      "datasets/celeba\n",
      "datasets/celeba\\training_test_splits/random_split/\n",
      "path= datasets/celeba\\img_align_celeba\n",
      "label_path= datasets/celeba\\list_attr_celeba.txt\n",
      "split_path= datasets/celeba\\training_test_splits/random_split/\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(dataset_folder,'training_test_splits/')):\n",
    "  print(dataset_folder)\n",
    "  print(os.path.join(dataset_folder,'training_test_splits/'))\n",
    "  os.mkdir(os.path.join(dataset_folder,'training_test_splits/'))\n",
    "\n",
    "if not os.path.exists(os.path.join(dataset_folder,'training_test_splits/random_split/')):\n",
    "  print(dataset_folder)\n",
    "  print(os.path.join(dataset_folder,'training_test_splits/random_split/'))\n",
    "  os.mkdir(os.path.join(dataset_folder,'training_test_splits/random_split/'))\n",
    "\n",
    "# Complete the data path\n",
    "path = os.path.join(dataset_folder,'img_align_celeba')\n",
    "label_path = os.path.join(dataset_folder,'list_attr_celeba.txt')\n",
    "split_path = os.path.join(dataset_folder,'training_test_splits/random_split/')\n",
    "\n",
    "print(\"path=\",path)\n",
    "print(\"label_path=\",label_path)\n",
    "print(\"split_path=\",split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oku-QmFY9f69"
   },
   "outputs": [],
   "source": [
    "#delimeter = [' ','\\t']\n",
    "import csv\n",
    "def load_labelfile(path):\n",
    "  labels = {}\n",
    "  with open(path, 'r') as csvfile:\n",
    "    lines = csv.reader(csvfile, delimiter='\\n')\n",
    "    for i,line in enumerate(lines):\n",
    "      #print(len(line))\n",
    "      #print(line)\n",
    "      #print(line[0].split(' ')[0])\n",
    "      if(i>=2):\n",
    "        #print(line[0].split()[1:])\n",
    "        labels[line[0].split()[0]] =  np.array(list(map(float,line[0].split()[1:])))\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0dOj8UIdkWJ",
    "outputId": "26f91d5c-db47-4cb8-aedb-fca35daa37cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('000003.jpg', array([-1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
      "       -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
      "        1.])), ('000004.jpg', array([-1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,\n",
      "       -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
      "        1.]))]\n"
     ]
    }
   ],
   "source": [
    "label_file = load_labelfile(label_path)\n",
    "#print(label_file[0])\n",
    "print(list(label_file.items())[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VRQAJA04lPND"
   },
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame.from_dict(label_file,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQeHfD9IoM8j",
    "outputId": "5590f195-087c-4a90-c9fa-c8e49b0f50c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0    1    2    3    4    5    6    7    8    9   ...   30   31  \\\n",
      "000001.jpg -1.0  1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0  1.0   \n",
      "000002.jpg -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0  ... -1.0  1.0   \n",
      "000003.jpg -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0   \n",
      "000004.jpg -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0   \n",
      "000005.jpg -1.0  1.0  1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0   \n",
      "\n",
      "             32   33   34   35   36   37   38   39  \n",
      "000001.jpg  1.0 -1.0  1.0 -1.0  1.0 -1.0 -1.0  1.0  \n",
      "000002.jpg -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0  \n",
      "000003.jpg -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0  \n",
      "000004.jpg  1.0 -1.0  1.0 -1.0  1.0  1.0 -1.0  1.0  \n",
      "000005.jpg -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0  1.0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(label_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UZxAOreQlmbt"
   },
   "outputs": [],
   "source": [
    "label_df_train, label_df_test = train_test_split(label_df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EQjHtvgpCWC",
    "outputId": "a2f5121c-ff15-48b8-982d-ee86830ce6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0    1    2    3    4    5    6    7    8    9   ...   30   31  \\\n",
      "108259.jpg -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0  1.0   \n",
      "120876.jpg -1.0 -1.0  1.0 -1.0 -1.0 -1.0  1.0  1.0 -1.0 -1.0  ... -1.0  1.0   \n",
      "147440.jpg -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  1.0  1.0 -1.0  ... -1.0  1.0   \n",
      "140419.jpg -1.0  1.0  1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  ... -1.0  1.0   \n",
      "098743.jpg  1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0   \n",
      "\n",
      "             32   33   34   35   36   37   38   39  \n",
      "108259.jpg -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0  1.0  \n",
      "120876.jpg -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0  1.0  \n",
      "147440.jpg  1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0  \n",
      "140419.jpg -1.0 -1.0  1.0 -1.0  1.0 -1.0 -1.0  1.0  \n",
      "098743.jpg  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(label_df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XRB5rcVzmTLH"
   },
   "outputs": [],
   "source": [
    "label_df_train.to_csv(split_path+'training.txt', sep=' ', index=True,header=False,mode='a')\n",
    "label_df_test.to_csv(split_path+'test.txt', sep=' ', index=True,header=False,mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jU0oxoLZ9aPE"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, folder_path, label_dict, split_file, image_size=256):\n",
    "    self.label_dict = label_dict\n",
    "    self.image_size = image_size\n",
    "    self.split = []\n",
    "    with open(split_file, 'r') as f:\n",
    "      for i in f.readlines():\n",
    "        self.split.append(i.strip().split()[0])\n",
    "    #print(\"split=\",self.split[0])\n",
    "\n",
    "    self.transform = transforms.Compose([\n",
    "\t\t\t\t        transforms.Resize(self.image_size),\n",
    "\t\t\t\t        transforms.CenterCrop(self.image_size),\n",
    "\t\t\t\t        transforms.ToTensor(),\n",
    "\t\t\t\t        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\t\t\t\t    ])\n",
    "\n",
    "    self.total_list = glob.glob(folder_path+'/*')\n",
    "    self.total_list = [i.replace(os.sep,'/') for i in self.total_list]\n",
    "    self.image_list = []\n",
    "\n",
    "    #print(\"total_list=\",self.total_list[0])\n",
    "\n",
    "    for i in tqdm(self.total_list):\n",
    "      #print(i)\n",
    "      #print(i.split('/')[-1])\n",
    "      if i.split('/')[-1] in self.split:\n",
    "        #print(\"yes\")\n",
    "        self.image_list.append(i)\n",
    "    #print(\"image_list=\",self.image_list[:3])\n",
    "    # Calculate len\n",
    "    self.data_len = len(self.image_list)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    single_image_path = self.image_list[index]\n",
    "\n",
    "    im_as_im = Image.open(single_image_path)\n",
    "\n",
    "    # Transform image to tensor, change data type\n",
    "    im_as_ten = self.transform(im_as_im)\n",
    "    #print(im_as_ten.shape)\n",
    "    # Get label(class) of the image based on the file name\n",
    "    label_idx = self.image_list[index].split('/')[-1]\n",
    "    #print(label_idx)\n",
    "\n",
    "    label = torch.Tensor(self.label_dict[label_idx])\n",
    "    return (im_as_ten.cuda(), label.cuda())\n",
    "    #return (im_as_ten, label)\n",
    "\n",
    "  def __len__(self):\n",
    "    #print(self.data_len)\n",
    "    return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4kbol6Wy9-pC"
   },
   "outputs": [],
   "source": [
    "def load_ckpt(path, model, optimizer):\n",
    "\t# ckpt = '/home/peiye/ImageEditing/scene_regressor/checkpoint/100_dict.model'\n",
    "\n",
    "\tckpt = torch.load(path)\n",
    "\tmodel.load_state_dict(ckpt['model'])\n",
    "\toptimizer.load_state_dict(ckpt['optm'])\n",
    "\treturn model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDrU6yVT-DwW",
    "outputId": "8aaddc1c-b61a-444a-b1bb-c2a6354f137c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 202599/202599 [05:52<00:00, 575.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 202599/202599 [03:18<00:00, 1019.43it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t#if not os.path.exists('./checkpoint_256'):\n",
    "\t#\tos.mkdir('./checkpoint_256')\n",
    "  #\tos.mkdir('./log_256')\n",
    "\n",
    "\t# Complete the data path\n",
    "\t#path = '/transient_scene/imageAlignedLD/'\n",
    "\t#label_path = '/transient_scene/annotations/annotations.tsv'\n",
    "\t#split_path = '/transient_scene/training_test_splits/random_split/'\n",
    "\n",
    "\n",
    "\tlabel_file = load_labelfile(label_path)\n",
    "\n",
    "\ttrain_data = CustomDataset(path, label_file,\n",
    "\t\t\t\t\t\t\t\tsplit_file=split_path+'training.txt',\n",
    "\t\t\t\t\t\t\t\timage_size=256)\n",
    "\n",
    "\ttest_data = CustomDataset(path, label_file,\n",
    "\t\t\t\t\t\t\t\tsplit_file=split_path+'test.txt',\n",
    "\t\t\t\t\t\t\t\timage_size=256)\n",
    "\n",
    "\ttrainloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\ttestloader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03bKVAxxFz-D",
    "outputId": "81198715-8745-4bc3-c277-1e1d5b3dd5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.5451, -0.5373, -0.5373,  ..., -0.4510, -0.4353, -0.4353],\n",
      "         [-0.5373, -0.5294, -0.5294,  ..., -0.4667, -0.4510, -0.4431],\n",
      "         [-0.5373, -0.5294, -0.5294,  ..., -0.4745, -0.4667, -0.4667],\n",
      "         ...,\n",
      "         [-0.5216, -0.4980, -0.4745,  ...,  0.5137,  0.5216,  0.5137],\n",
      "         [-0.4902, -0.4745, -0.4510,  ...,  0.6078,  0.6078,  0.5922],\n",
      "         [-0.4667, -0.4431, -0.4196,  ...,  0.6627,  0.6627,  0.6471]],\n",
      "\n",
      "        [[-0.6078, -0.6000, -0.6000,  ..., -0.4667, -0.4588, -0.4588],\n",
      "         [-0.6000, -0.5922, -0.5922,  ..., -0.4824, -0.4824, -0.4824],\n",
      "         [-0.6000, -0.5922, -0.5922,  ..., -0.5059, -0.5059, -0.5059],\n",
      "         ...,\n",
      "         [-0.4353, -0.4118, -0.3804,  ...,  0.5373,  0.5529,  0.5608],\n",
      "         [-0.4039, -0.3882, -0.3569,  ...,  0.6235,  0.6392,  0.6471],\n",
      "         [-0.3804, -0.3569, -0.3255,  ...,  0.6706,  0.6784,  0.6863]],\n",
      "\n",
      "        [[-0.6314, -0.6235, -0.6235,  ..., -0.5059, -0.5059, -0.5137],\n",
      "         [-0.6235, -0.6157, -0.6157,  ..., -0.5216, -0.5216, -0.5294],\n",
      "         [-0.6235, -0.6157, -0.6157,  ..., -0.5529, -0.5529, -0.5529],\n",
      "         ...,\n",
      "         [-0.4039, -0.3804, -0.3490,  ...,  0.5686,  0.5843,  0.5843],\n",
      "         [-0.3725, -0.3569, -0.3255,  ...,  0.6471,  0.6627,  0.6627],\n",
      "         [-0.3490, -0.3255, -0.3020,  ...,  0.6863,  0.7020,  0.7098]]],\n",
      "       device='cuda:0'), tensor([-1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
      "        -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "#print(train_data.__len__())\n",
    "#print(test_data.__len__())\n",
    "#print(next(iter(trainloader))[0].shape)\n",
    "print(train_data.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMZ-do_xE0AU",
    "outputId": "88cac334-5c41-4106-b131-e7c77169a76f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sanika/.cache\\torch\\hub\\pytorch_vision_v0.5.0\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  3 ITER:  4242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 2; Loss: 0.24551: 100%|███████████████████████████████████████████████████| 3/3 [1:29:08<00:00, 1782.81s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  files = glob.glob('./log_256/*')\n",
    "  for f in files:\n",
    "      os.remove(f)\n",
    "  os.rmdir('./log_256')\n",
    "  files = glob.glob('./checkpoint_256/*')\n",
    "  for f in files:\n",
    "      os.remove(f)\n",
    "  os.rmdir('./checkpoint_256')\n",
    "    \n",
    "  if not os.path.exists('./checkpoint_256'):\n",
    "      os.mkdir('./checkpoint_256')\n",
    "      os.mkdir('./log_256')\n",
    "\n",
    "  #model = torch.hub.load('pytorch/vision:v0.5.0', 'resnet50', pretrained=True)\n",
    "  #print(model)\n",
    "  model = torch.hub.load('pytorch/vision:v0.5.0', 'mobilenet_v2', pretrained=True)\n",
    "  #print(model)\n",
    "  model.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2, inplace=False),\n",
    "            torch.nn.Linear(in_features=1280, out_features=40, bias=True)\n",
    "        )\n",
    "  #print(model)\n",
    "\n",
    "  model = model.cuda()\n",
    "\n",
    "  N_EPOCH = 3\n",
    "  N_ITER = len(trainloader)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "  # model, optimizer = load_ckpt(path='/home/peiye/ImageEditing/scene_regressor/checkpoint/200_dict.model',\n",
    "  # \t\t\t\t\t\t\t model=model,\n",
    "  # \t\t\t\t\t\t\t optimizer=optimizer)\n",
    "\n",
    "  writer = SummaryWriter(log_dir='./log_256'+'/')\n",
    "\n",
    "  criterion = nn.MSELoss().cuda()\n",
    "  print('EPOCH: ', N_EPOCH, 'ITER: ', N_ITER)\n",
    "\n",
    "  pbar = tqdm(range(N_EPOCH))\n",
    "\n",
    "  for epoch in pbar:\n",
    "    it = iter(trainloader)\n",
    "    for iter_ in range(N_ITER):\n",
    "      data, label = next(it) # [32, 3, 256, 256]\n",
    "      preds = model(data)\n",
    "      optimizer.zero_grad()\n",
    "      #print(label.shape)\n",
    "      #print(preds.shape)\n",
    "      Loss = criterion(preds, label).mean()\n",
    "      Loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      pbar.set_description(f'Iter {iter_ + 1} Loss: {Loss:.5f}')\n",
    "      if (N_ITER* epoch + iter_) % 50 == 0:\n",
    "        writer.add_scalar('Train/Loss', Loss, N_ITER*epoch + iter_)\n",
    "\n",
    "\t\t# if epoch % 5 == 0 and epoch != 0:\n",
    "\n",
    "    if epoch % 1 == 0 and epoch != 0:\n",
    "      # Test\n",
    "      with torch.no_grad():\n",
    "        test_loss = []\n",
    "        # aps = []\n",
    "\n",
    "        for test_data, test_label in testloader:\n",
    "          test_preds = model(test_data)\n",
    "          test_loss.append(criterion(test_preds, test_label).mean().cpu().item())\n",
    "\n",
    "\t\t\t\t\t# print(test_preds.cpu().numpy().shape,test_label.cpu().numpy().shape)\n",
    "\t\t\t\t\t# ap = average_precision_score(test_preds.cpu().numpy(),test_label.cpu().numpy())\n",
    "\t\t\t\t\t# aps.append(ap)\n",
    "\t\t\t\t\t# break\n",
    "\n",
    "        pbar.set_description(f'Test epoch {epoch}; Loss: {np.mean(test_loss):.5f}')\n",
    "        writer.add_scalar('Test/MSE', np.mean(test_loss), epoch)\n",
    "\t\t\t\t# writer.add_scalar('Test/AP', np.mean(aps), epoch)\n",
    "\n",
    "\n",
    "\t\t# Save model\n",
    "    torch.save({\n",
    "              'model': model.state_dict(),\n",
    "              'optm': optimizer.state_dict()},\n",
    "               'checkpoint_256' + f'/{str(epoch + 1).zfill(3)}_dict.model')\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
